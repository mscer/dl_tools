{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##trans probability; (s a s')\n",
    "T = np.array([\n",
    "    [[0.7,0.3,0],[1.0,0.0,0.0],[0.8,0.2,0.0]],\n",
    "    [[0.0,1.0,0.0],[nan,nan,nan],[0.0,0.0,1.0]],\n",
    "    [[nan,nan,nan],[0.8,0.1,0.1],[nan,nan,nan]]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##reward (s a s')\n",
    "R = np.array([\n",
    "    [[10.,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]],\n",
    "    [[10.,0.0,0.0],[nan,nan,nan],[0.0, 0.0, -50.]],\n",
    "    [[nan, nan, nan], [40., 0.0, 0.0], [nan, nan, nan]],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##action of every state\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state ->action 3*3\n",
    "Q = np.full((3,3),-np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Q-value iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "discount_rate = 0.95\n",
    "n_iterations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "## init\n",
    "for state,action in enumerate(possible_actions):\n",
    "    Q[state,action] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangjiaqi/workingspace/anaconda3/envs/ml3/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "##q = sum_of_s' tran_probability*(reward+discount* max(q_s'))\n",
    "for iteration  in range(n_iterations):\n",
    "    Q_prev = Q.copy()\n",
    "    ##每个state 每个action 分别迭代Q\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            ##s 采取action，可能到达的任何状态\n",
    "            Q[s,a] = np.sum(T[s,a,ns] *\n",
    "                            (R[s,a,ns] + np.max(Q_prev[ns])*discount_rate)\n",
    "                            for ns in range(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21.88646117, 20.79149867, 16.854807  ],\n",
       "       [ 1.10804034,        -inf,  1.16703135],\n",
       "       [       -inf, 53.8607061 ,        -inf]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##重点是长期收益\n",
    "Q## 每个action都是长期受益的期望"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// Qnew(s,a) = (1-a)*Qold(s,a) + a(r + discount * max(Qold(s')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate0 = 0.05\n",
    "learning_rate_decay = 0.1\n",
    "n_iterations = 20000\n",
    "discount_rate = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.full((3,3),-np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "##init\n",
    "for state_index,action_index in enumerate(possible_actions):\n",
    "    Q[state_index,action_index] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "for iteration in range(n_iterations):\n",
    "    #print(iteration)\n",
    "    ##随机抽取action\n",
    "    action = rnd.choice(possible_actions[s])\n",
    "    ##get state\n",
    "    ns = rnd.choice(range(3),p=T[s,action])\n",
    "    ##get reward\n",
    "    reward = R[s,action,ns]\n",
    "    ##update\n",
    "    learning_rate = learning_rate0 / (1 + iteration * learning_rate_decay)\n",
    "    Q[s,action] = learning_rate*Q[s,action] + (1-learning_rate)*(reward+ discount_rate*np.max(Q[ns]))\n",
    "    s = ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 98.30833514,  88.30844427,  80.51824726],\n",
       "       [ 80.51794796,         -inf,  79.91865869],\n",
       "       [        -inf, 136.75635559,         -inf]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##重点是长期收益\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd.choice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
