{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.kerasras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data &feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000 \n",
    "maxlen = 200  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangjiaqi/workingspace/anaconda3/envs/ml3/lib/python3.7/site-packages/tensorflow_core/python/keras/datasets/imdb.py:129: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/Users/zhangjiaqi/workingspace/anaconda3/envs/ml3/lib/python3.7/site-packages/tensorflow_core/python/keras/datasets/imdb.py:130: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Training sequences\n",
      "25000 Validation sequences\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "##transformer= \n",
    "## xa = layerNorm(multi_head_attention(x)+x)\n",
    "## xb = layerNorm(MLP(xa)+xa)\n",
    "## embed = token_embed + poistion_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,Dense,Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.transpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reshape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.softmax?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention_keras(keras.layers.Layer):\n",
    "    '''\n",
    "    process inputs by multi_head_attention\n",
    "    inputs:[batch_size,seq_len,embed_dim]\n",
    "    embed_dim = num_heads*embed_dim2\n",
    "    self_attention[batch_size,num_heads,seq_len,seq_len]\n",
    "    query,key,value:[batch_size,num_head,seq_len,embed_dim2]\n",
    "    '''\n",
    "    def __init__(self,embed_dim,num_heads = 10):\n",
    "        super(MultiHeadAttention_keras,self).__init__()\n",
    "        if(embed_dim%num_heads !=0):\n",
    "            print('un match dim')\n",
    "            return\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        ##query/key/value trans\n",
    "        self.project_dim = self.embed_dim//num_heads\n",
    "        ##trans [batch_size,seq_len,embed_dim] to [batch_size,seq_len,project_dim*num_heads]\n",
    "        self.query = keras.layers.Dense(units=self.project_dim*self.num_heads,name='query_mlp')\n",
    "        self.key = keras.layers.Dense(units=self.project_dim*self.num_heads,name='key_mlp')\n",
    "        self.value = keras.layers.Dense(units=self.project_dim*self.num_heads,name='value_mlp')\n",
    "        ##concat and trans\n",
    "        self.concat = keras.layers.Dense(units=self.project_dim*self.num_heads,name='concat_head')\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        ##inputs:[batch_size,seq_len,embed_dim]\n",
    "        query = self.key(inputs)\n",
    "        key = self.key(inputs)\n",
    "        value = self.value(inputs)\n",
    "        \n",
    "        ##query,key,value:[batch_size,seq_len,project_dim*num_heads]\n",
    "        query = self.split_head(query)\n",
    "        key = self.split_head(key)\n",
    "        value = self.split_head(value)\n",
    "        \n",
    "        ## compute self-attention\n",
    "        att_out = self.self_attention(query,key,value)#[batch_size,num_heads,seq_len,embed_dim2]\n",
    "        att_out = tf.transpose(att_out,perm=[0,2,1,3])\n",
    "        \n",
    "        new_shape = tf.shape(inputs)\n",
    "        print(new_shape)\n",
    "        att_out = tf.reshape(att_out,shape=new_shape)\n",
    "        out = self.concat(att_out)\n",
    "        return out\n",
    "        \n",
    "    def split_head(self,x):\n",
    "        ##x:[batch_size,seq_len,num_heads*embed_dim2]\n",
    "        #new_shape = tf.shape(x)[:-1]#+tf.cast([self.num_heads,self.project_dim],tf.int32)\n",
    "        new_shape=tf.cast([tf.shape(x)[0],tf.shape(x)[1],self.num_heads,self.project_dim],tf.int32)\n",
    "        #print(new_shape)\n",
    "        x = tf.reshape(x,new_shape) ##[batch_size,seq_len,num_heads,embed_dim2]\n",
    "        x = tf.transpose(x,perm=[0,2,1,3])\n",
    "        return x\n",
    "    \n",
    "    def self_attention(self,query,key,value):\n",
    "        ##x:[batch_size,num_heads,seq_len,embed_dim2]\n",
    "        attention_weight = tf.matmul(query,key,transpose_b=True)\n",
    "        ##attentin_weight:[batch_size,num_heads,seq_len,seq_len]\n",
    "        ##scaled-dot-product\n",
    "        #print(attention_weight)\n",
    "        scaled_ratio = tf.sqrt(tf.cast(tf.shape(query)[-1],tf.float32))\n",
    "        attention_weight /= scaled_ratio\n",
    "        ##attention_weight:[batch_size,num_heads,seq_len,seq_len]\n",
    "        attention_weight = tf.nn.softmax(attention_weight,axis=-1)\n",
    "        \n",
    "        ##get attention_value\n",
    "        att_out = tf.matmul(attention_weight,value)\n",
    "        return att_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = np.random.random_sample((100,20,40)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"multi_head_attention_keras_1/Shape_7:0\", shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "att_query = MultiHeadAttention_keras(40,num_heads=10)(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "##transformer block:\n",
    "## xa = layerNorm(multi_head_attention(x)+x)\n",
    "## xb = layerNorm(MLP(xa)+xa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self,embed_dim,num_heads,mlp_num,keep_prob = 0.1):\n",
    "        super(TransformerBlock,self).__init__()\n",
    "        self.att = MultiHeadAttention_keras(embed_dim,num_heads)\n",
    "        self.ln1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ln2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.mlp = keras.models.Sequential([\n",
    "            keras.layers.Dense(units=mlp_num,activation='relu'),\n",
    "            keras.layers.Dense(units=embed_dim)\n",
    "        ])\n",
    "        \n",
    "        self.dropout1 = keras.layers.Dropout(keep_prob)\n",
    "        self.dropout2 = keras.layers.Dropout(keep_prob)\n",
    "    \n",
    "    def call(self,inputs,training=True):\n",
    "        ##[batch_size,seq_len,embed_dim]\n",
    "        att_out = self.att(inputs)\n",
    "        att_out = self.dropout1(att_out,training=training)\n",
    "        out1 = self.ln1(inputs+att_out)\n",
    "        mlp_out = self.mlp(out1)\n",
    "        mlp_out = self.dropout2(mlp_out,training=training)\n",
    "        out2 = self.ln2(mlp_out+out1)\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"transformer_block_1/multi_head_attention_keras_2/Shape_7:0\", shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "query = np.random.random_sample((100,20,40)).astype(np.float32)\n",
    "trans_query = TransformerBlock(embed_dim=40,num_heads=10,mlp_num=20)(query,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "##token_embed and position_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbed(keras.layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbed, self).__init__()\n",
    "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "##transformer= \n",
    "## xa = layerNorm(multi_head_attention(x)+x)\n",
    "## xb = layerNorm(MLP(xa)+xa)\n",
    "## embed = token_embed + poistion_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32 \n",
    "num_heads = 2  \n",
    "mlp_dim = 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"transformer_block_2/multi_head_attention_keras_3/Shape_7:0\", shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbed(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, mlp_dim)\n",
    "x = transformer_block(x)\n",
    "x = keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = keras.layers.Dropout(0.1)(x)\n",
    "x = keras.layers.Dense(20, activation=\"relu\")(x)\n",
    "x = keras.layers.Dropout(0.1)(x)\n",
    "outputs = keras.layers.Dense(2, activation=\"softmax\")(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "token_and_position_embed_1 ( (None, 200, 32)           646400    \n",
      "_________________________________________________________________\n",
      "transformer_block_2 (Transfo (None, 200, 32)           5408      \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 652,510\n",
      "Trainable params: 652,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 386s 15ms/sample - loss: 0.3868 - acc: 0.8117 - val_loss: 0.3023 - val_acc: 0.8718\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 442s 18ms/sample - loss: 0.2036 - acc: 0.9222 - val_loss: 0.3201 - val_acc: 0.8692\n"
     ]
    }
   ],
   "source": [
    "nn.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = nn.fit(\n",
    "    x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_2\n",
      "token_and_position_embed_1\n",
      "transformer_block_2\n",
      "global_average_pooling1d_1\n",
      "dropout_8\n",
      "dense_8\n",
      "dropout_9\n",
      "dense_9\n"
     ]
    }
   ],
   "source": [
    "for layer in nn.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
