{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##cross_method for rl\n",
    "## get episode ,and learn to map state to action, which ignore accurate reward or gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_obs(obs):\n",
    "    obs_np = np.zeros([16])\n",
    "    obs_np[obs] = 1\n",
    "    return obs_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_obs(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    '''\n",
    "    cross method for map state to action\n",
    "    '''\n",
    "    def __init__(self,obs_size,hidden_size,action_size):\n",
    "        super(Net,self).__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(obs_size,hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_size,action_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 交互过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode = namedtuple('Eposide',field_names=['reward','step']) ## filter to get good eposide\n",
    "EpisodeStep = namedtuple('EposideStep',field_names=['observation','action']) ## map obs to action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batch(env,net,batch_size):\n",
    "    batch = []\n",
    "    ##action ->reward \n",
    "    episode_reward = 0.0\n",
    "    episode_step = []\n",
    "    obs = env.reset()\n",
    "    op_sm = torch.nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        ##get action\n",
    "        obs_value = torch.FloatTensor([one_hot_obs(obs)])\n",
    "        action_prob = op_sm(net(obs_value)).data.numpy()[0]\n",
    "        action = np.random.choice(len(action_prob),p = action_prob)\n",
    "        ##get obs and reward\n",
    "        next_obs , reward, is_done,_ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_step.append(EpisodeStep(one_hot_obs(obs),action))\n",
    "        if is_done:\n",
    "            batch.append(Episode(episode_reward,episode_step))\n",
    "            episode_reward = 0.0\n",
    "            episode_step = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 学习过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch,percentile,discount):\n",
    "    reward = list(map(lambda x:x.reward*(discount**len(x.step)),batch))\n",
    "    #print(reward)\n",
    "    reward_bound = np.percentile(reward,percentile)\n",
    "    #print(reward_bound)\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    elite_batch = []#保留good episode\n",
    "    for one_reward,example in zip(reward,batch):\n",
    "        if one_reward <= reward_bound:\n",
    "            continue\n",
    "        train_obs.extend((map(lambda x:x.observation,example.step)))\n",
    "        train_act.extend((map(lambda x:x.action,example.step)))\n",
    "        elite_batch.append(example)\n",
    "        \n",
    "    return elite_batch,train_obs,train_act,reward_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN = 128\n",
    "PERCERTILE = 30\n",
    "DISCOUNT = 0.9\n",
    "BATCHS_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_exp =1\n",
    "writer = SummaryWriter('runs/cross_entroy_fl/exp%d'%(num_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 :loss:1.295987, reward_bound:0.000000, reward_mean:0.040000,batch=204\n",
      "199 :loss:1.157771, reward_bound:0.313811, reward_mean:0.030000,batch=224\n",
      "299 :loss:1.034672, reward_bound:0.161254, reward_mean:0.040000,batch=227\n",
      "399 :loss:1.006526, reward_bound:0.350616, reward_mean:0.080000,batch=228\n",
      "499 :loss:0.977155, reward_bound:0.366422, reward_mean:0.090000,batch=229\n",
      "599 :loss:0.951341, reward_bound:0.205891, reward_mean:0.070000,batch=225\n",
      "699 :loss:0.944008, reward_bound:0.423771, reward_mean:0.050000,batch=230\n",
      "799 :loss:0.822725, reward_bound:0.291844, reward_mean:0.120000,batch=225\n",
      "899 :loss:0.794710, reward_bound:0.250233, reward_mean:0.050000,batch=230\n",
      "999 :loss:0.777797, reward_bound:0.251645, reward_mean:0.100000,batch=220\n",
      "1099 :loss:0.752025, reward_bound:0.307534, reward_mean:0.070000,batch=229\n",
      "1199 :loss:0.754134, reward_bound:0.331245, reward_mean:0.100000,batch=221\n",
      "1299 :loss:0.743693, reward_bound:0.338218, reward_mean:0.130000,batch=231\n",
      "1399 :loss:0.684448, reward_bound:0.044275, reward_mean:0.100000,batch=223\n",
      "1499 :loss:0.697532, reward_bound:0.383546, reward_mean:0.170000,batch=227\n",
      "1599 :loss:0.694827, reward_bound:0.404639, reward_mean:0.150000,batch=230\n",
      "1699 :loss:0.692758, reward_bound:0.313811, reward_mean:0.140000,batch=224\n",
      "1799 :loss:0.698082, reward_bound:0.157136, reward_mean:0.130000,batch=220\n",
      "1899 :loss:0.682768, reward_bound:0.348678, reward_mean:0.190000,batch=228\n",
      "1999 :loss:0.635227, reward_bound:0.157733, reward_mean:0.150000,batch=208\n",
      "2099 :loss:0.620560, reward_bound:0.185302, reward_mean:0.220000,batch=219\n",
      "2199 :loss:0.636331, reward_bound:0.417553, reward_mean:0.200000,batch=231\n",
      "2299 :loss:0.649098, reward_bound:0.289054, reward_mean:0.160000,batch=222\n",
      "2399 :loss:0.637563, reward_bound:0.205891, reward_mean:0.210000,batch=225\n",
      "2499 :loss:0.644415, reward_bound:0.177890, reward_mean:0.180000,batch=219\n",
      "2699 :loss:0.579584, reward_bound:0.282430, reward_mean:0.170000,batch=226\n",
      "2799 :loss:0.562708, reward_bound:0.254187, reward_mean:0.250000,batch=216\n",
      "2899 :loss:0.559569, reward_bound:0.313811, reward_mean:0.180000,batch=226\n",
      "2999 :loss:0.562399, reward_bound:0.478297, reward_mean:0.160000,batch=151\n",
      "3099 :loss:0.561110, reward_bound:0.211382, reward_mean:0.150000,batch=226\n",
      "3199 :loss:0.455065, reward_bound:0.000000, reward_mean:0.160000,batch=58\n",
      "3299 :loss:0.526090, reward_bound:0.246561, reward_mean:0.240000,batch=224\n",
      "3399 :loss:0.518243, reward_bound:0.310882, reward_mean:0.210000,batch=226\n",
      "3499 :loss:0.506818, reward_bound:0.364175, reward_mean:0.250000,batch=230\n",
      "3599 :loss:0.490320, reward_bound:0.449599, reward_mean:0.280000,batch=230\n",
      "3699 :loss:0.480421, reward_bound:0.254187, reward_mean:0.200000,batch=222\n",
      "3799 :loss:0.489541, reward_bound:0.430467, reward_mean:0.270000,batch=229\n",
      "3899 :loss:0.430056, reward_bound:0.185302, reward_mean:0.240000,batch=210\n",
      "3999 :loss:0.461753, reward_bound:0.387420, reward_mean:0.260000,batch=225\n",
      "4099 :loss:0.485642, reward_bound:0.278036, reward_mean:0.260000,batch=223\n",
      "4199 :loss:0.448762, reward_bound:0.201773, reward_mean:0.230000,batch=215\n",
      "4299 :loss:0.451632, reward_bound:0.307848, reward_mean:0.230000,batch=227\n",
      "4399 :loss:0.389662, reward_bound:0.313811, reward_mean:0.360000,batch=201\n",
      "4499 :loss:0.365474, reward_bound:0.282430, reward_mean:0.280000,batch=225\n",
      "4599 :loss:0.373419, reward_bound:0.313811, reward_mean:0.240000,batch=218\n",
      "4699 :loss:0.373307, reward_bound:0.430467, reward_mean:0.290000,batch=229\n",
      "4799 :loss:0.371712, reward_bound:0.310672, reward_mean:0.240000,batch=227\n",
      "4899 :loss:0.341750, reward_bound:0.254187, reward_mean:0.220000,batch=219\n",
      "4999 :loss:0.315085, reward_bound:0.348678, reward_mean:0.370000,batch=223\n",
      "5099 :loss:0.314100, reward_bound:0.265484, reward_mean:0.270000,batch=216\n",
      "5199 :loss:0.295798, reward_bound:0.166772, reward_mean:0.280000,batch=220\n",
      "5299 :loss:0.284919, reward_bound:0.341705, reward_mean:0.260000,batch=222\n",
      "5399 :loss:0.283529, reward_bound:0.205891, reward_mean:0.380000,batch=215\n",
      "5499 :loss:0.268147, reward_bound:0.205891, reward_mean:0.240000,batch=210\n",
      "5599 :loss:0.243066, reward_bound:0.071790, reward_mean:0.320000,batch=218\n",
      "5699 :loss:0.257950, reward_bound:0.282430, reward_mean:0.270000,batch=223\n",
      "5799 :loss:0.266743, reward_bound:0.313811, reward_mean:0.400000,batch=213\n",
      "5899 :loss:0.225058, reward_bound:0.228768, reward_mean:0.370000,batch=184\n",
      "5999 :loss:0.200217, reward_bound:0.414109, reward_mean:0.370000,batch=229\n",
      "6099 :loss:0.192627, reward_bound:0.254281, reward_mean:0.350000,batch=225\n",
      "6199 :loss:0.203449, reward_bound:0.360301, reward_mean:0.400000,batch=225\n",
      "6299 :loss:0.194552, reward_bound:0.348678, reward_mean:0.390000,batch=208\n",
      "6399 :loss:0.196535, reward_bound:0.348678, reward_mean:0.310000,batch=221\n",
      "6499 :loss:0.202312, reward_bound:0.417553, reward_mean:0.240000,batch=231\n",
      "6599 :loss:0.208010, reward_bound:0.098477, reward_mean:0.300000,batch=210\n",
      "6699 :loss:0.205706, reward_bound:0.254187, reward_mean:0.290000,batch=220\n",
      "6799 :loss:0.197024, reward_bound:0.233852, reward_mean:0.170000,batch=220\n",
      "6899 :loss:0.186433, reward_bound:0.313811, reward_mean:0.340000,batch=212\n",
      "6999 :loss:0.189401, reward_bound:0.375798, reward_mean:0.290000,batch=231\n",
      "7099 :loss:0.179202, reward_bound:0.268308, reward_mean:0.340000,batch=221\n",
      "7199 :loss:0.153011, reward_bound:0.254187, reward_mean:0.280000,batch=221\n",
      "7299 :loss:0.154660, reward_bound:0.148694, reward_mean:0.260000,batch=212\n",
      "7399 :loss:0.152799, reward_bound:0.352553, reward_mean:0.260000,batch=229\n",
      "7499 :loss:0.143180, reward_bound:0.371924, reward_mean:0.340000,batch=226\n",
      "7599 :loss:0.153509, reward_bound:0.117929, reward_mean:0.260000,batch=203\n",
      "7699 :loss:0.170383, reward_bound:0.205891, reward_mean:0.320000,batch=227\n",
      "7799 :loss:0.196544, reward_bound:0.352553, reward_mean:0.310000,batch=229\n",
      "7899 :loss:0.180459, reward_bound:0.387420, reward_mean:0.350000,batch=218\n",
      "7999 :loss:0.172518, reward_bound:0.413249, reward_mean:0.370000,batch=226\n",
      "8099 :loss:0.163394, reward_bound:0.387420, reward_mean:0.300000,batch=231\n",
      "8199 :loss:0.174924, reward_bound:0.313811, reward_mean:0.270000,batch=227\n",
      "8299 :loss:0.169604, reward_bound:0.345192, reward_mean:0.350000,batch=227\n",
      "8399 :loss:0.168601, reward_bound:0.254187, reward_mean:0.360000,batch=222\n",
      "8499 :loss:0.177523, reward_bound:0.313811, reward_mean:0.260000,batch=225\n",
      "8599 :loss:0.148215, reward_bound:0.135085, reward_mean:0.270000,batch=212\n",
      "8699 :loss:0.144703, reward_bound:0.285568, reward_mean:0.340000,batch=222\n",
      "8799 :loss:0.163249, reward_bound:0.088629, reward_mean:0.310000,batch=210\n",
      "8899 :loss:0.180029, reward_bound:0.205891, reward_mean:0.360000,batch=217\n",
      "8999 :loss:0.188764, reward_bound:0.364175, reward_mean:0.320000,batch=230\n",
      "9099 :loss:0.193647, reward_bound:0.282430, reward_mean:0.350000,batch=218\n",
      "9199 :loss:0.191289, reward_bound:0.478297, reward_mean:0.330000,batch=225\n",
      "9299 :loss:0.186592, reward_bound:0.228768, reward_mean:0.360000,batch=219\n",
      "9399 :loss:0.192617, reward_bound:0.166772, reward_mean:0.280000,batch=211\n",
      "9499 :loss:0.190844, reward_bound:0.216369, reward_mean:0.260000,batch=222\n",
      "9599 :loss:0.190784, reward_bound:0.254187, reward_mean:0.370000,batch=213\n",
      "9699 :loss:0.194780, reward_bound:0.404639, reward_mean:0.330000,batch=230\n",
      "9799 :loss:0.140886, reward_bound:0.000000, reward_mean:0.300000,batch=149\n",
      "9899 :loss:0.175563, reward_bound:0.282430, reward_mean:0.410000,batch=220\n",
      "9999 :loss:0.173732, reward_bound:0.254187, reward_mean:0.400000,batch=217\n",
      "10099 :loss:0.177570, reward_bound:0.241477, reward_mean:0.350000,batch=221\n",
      "10199 :loss:0.170687, reward_bound:0.254187, reward_mean:0.330000,batch=206\n",
      "10299 :loss:0.160389, reward_bound:0.276732, reward_mean:0.340000,batch=223\n",
      "10399 :loss:0.150111, reward_bound:0.090708, reward_mean:0.350000,batch=208\n",
      "10499 :loss:0.120886, reward_bound:0.016423, reward_mean:0.340000,batch=189\n",
      "10599 :loss:0.152366, reward_bound:0.360301, reward_mean:0.320000,batch=225\n",
      "10699 :loss:0.123778, reward_bound:0.282430, reward_mean:0.370000,batch=216\n",
      "10799 :loss:0.125355, reward_bound:0.320784, reward_mean:0.270000,batch=227\n",
      "10899 :loss:0.128490, reward_bound:0.478297, reward_mean:0.330000,batch=200\n",
      "10999 :loss:0.116230, reward_bound:0.208179, reward_mean:0.330000,batch=222\n",
      "11099 :loss:0.126648, reward_bound:0.166772, reward_mean:0.270000,batch=214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11199 :loss:0.118165, reward_bound:0.228768, reward_mean:0.270000,batch=210\n",
      "11299 :loss:0.127864, reward_bound:0.426163, reward_mean:0.410000,batch=227\n",
      "11399 :loss:0.132657, reward_bound:0.324271, reward_mean:0.280000,batch=225\n",
      "11499 :loss:0.131567, reward_bound:0.328533, reward_mean:0.290000,batch=227\n",
      "11599 :loss:0.123440, reward_bound:0.228768, reward_mean:0.390000,batch=206\n",
      "11699 :loss:0.123041, reward_bound:0.185302, reward_mean:0.320000,batch=216\n",
      "11799 :loss:0.126013, reward_bound:0.430467, reward_mean:0.350000,batch=231\n",
      "11899 :loss:0.128785, reward_bound:0.348678, reward_mean:0.250000,batch=225\n",
      "11999 :loss:0.118884, reward_bound:0.285568, reward_mean:0.350000,batch=229\n",
      "12099 :loss:0.138468, reward_bound:0.387420, reward_mean:0.260000,batch=231\n",
      "12299 :loss:0.141626, reward_bound:0.334731, reward_mean:0.350000,batch=226\n",
      "12399 :loss:0.147683, reward_bound:0.348678, reward_mean:0.300000,batch=227\n",
      "12499 :loss:0.150032, reward_bound:0.185302, reward_mean:0.360000,batch=202\n",
      "12599 :loss:0.158712, reward_bound:0.387420, reward_mean:0.330000,batch=224\n",
      "12699 :loss:0.154930, reward_bound:0.478297, reward_mean:0.290000,batch=213\n",
      "12799 :loss:0.167039, reward_bound:0.205891, reward_mean:0.350000,batch=212\n",
      "12899 :loss:0.160291, reward_bound:0.154097, reward_mean:0.400000,batch=198\n",
      "12999 :loss:0.149367, reward_bound:0.185302, reward_mean:0.240000,batch=210\n",
      "13099 :loss:0.151572, reward_bound:0.282430, reward_mean:0.340000,batch=212\n",
      "13199 :loss:0.142458, reward_bound:0.136586, reward_mean:0.340000,batch=215\n",
      "13299 :loss:0.150445, reward_bound:0.322179, reward_mean:0.280000,batch=226\n",
      "13399 :loss:0.145480, reward_bound:0.294982, reward_mean:0.280000,batch=223\n",
      "13499 :loss:0.139685, reward_bound:0.254187, reward_mean:0.260000,batch=216\n",
      "13599 :loss:0.127801, reward_bound:0.313811, reward_mean:0.290000,batch=220\n",
      "13699 :loss:0.135196, reward_bound:0.254187, reward_mean:0.370000,batch=218\n",
      "13799 :loss:0.131828, reward_bound:0.313811, reward_mean:0.260000,batch=225\n",
      "13899 :loss:0.134934, reward_bound:0.307848, reward_mean:0.360000,batch=227\n",
      "13999 :loss:0.154335, reward_bound:0.094538, reward_mean:0.300000,batch=205\n",
      "14099 :loss:0.143889, reward_bound:0.231310, reward_mean:0.360000,batch=215\n",
      "14199 :loss:0.145422, reward_bound:0.185302, reward_mean:0.410000,batch=216\n",
      "14299 :loss:0.149925, reward_bound:0.224421, reward_mean:0.270000,batch=220\n",
      "14399 :loss:0.145631, reward_bound:0.265484, reward_mean:0.370000,batch=223\n",
      "14499 :loss:0.148373, reward_bound:0.324271, reward_mean:0.330000,batch=225\n",
      "14599 :loss:0.162017, reward_bound:0.345192, reward_mean:0.300000,batch=227\n",
      "14699 :loss:0.164112, reward_bound:0.282430, reward_mean:0.350000,batch=220\n",
      "14799 :loss:0.150228, reward_bound:0.262659, reward_mean:0.320000,batch=218\n",
      "14899 :loss:0.161582, reward_bound:0.414109, reward_mean:0.270000,batch=229\n",
      "14999 :loss:0.178503, reward_bound:0.430467, reward_mean:0.340000,batch=229\n",
      "15099 :loss:0.173354, reward_bound:0.387420, reward_mean:0.240000,batch=231\n",
      "15199 :loss:0.137044, reward_bound:0.254187, reward_mean:0.310000,batch=215\n",
      "15299 :loss:0.141282, reward_bound:0.430467, reward_mean:0.290000,batch=179\n",
      "15399 :loss:0.145840, reward_bound:0.163603, reward_mean:0.270000,batch=206\n",
      "15499 :loss:0.141416, reward_bound:0.227962, reward_mean:0.310000,batch=224\n",
      "15599 :loss:0.145702, reward_bound:0.348678, reward_mean:0.240000,batch=229\n",
      "15699 :loss:0.134368, reward_bound:0.430467, reward_mean:0.330000,batch=219\n",
      "15799 :loss:0.144531, reward_bound:0.071790, reward_mean:0.320000,batch=198\n",
      "15899 :loss:0.144814, reward_bound:0.219617, reward_mean:0.250000,batch=226\n",
      "15999 :loss:0.152339, reward_bound:0.331245, reward_mean:0.280000,batch=228\n",
      "16099 :loss:0.165016, reward_bound:0.313811, reward_mean:0.290000,batch=227\n",
      "16199 :loss:0.159831, reward_bound:0.387420, reward_mean:0.270000,batch=226\n",
      "16299 :loss:0.147168, reward_bound:0.294982, reward_mean:0.250000,batch=230\n",
      "16399 :loss:0.166139, reward_bound:0.282430, reward_mean:0.290000,batch=223\n",
      "16499 :loss:0.165741, reward_bound:0.197655, reward_mean:0.340000,batch=219\n",
      "16599 :loss:0.143314, reward_bound:0.294982, reward_mean:0.270000,batch=223\n",
      "16699 :loss:0.148015, reward_bound:0.282430, reward_mean:0.320000,batch=214\n",
      "16799 :loss:0.149938, reward_bound:0.334731, reward_mean:0.300000,batch=226\n",
      "16899 :loss:0.138991, reward_bound:0.282430, reward_mean:0.350000,batch=185\n",
      "16999 :loss:0.135344, reward_bound:0.166772, reward_mean:0.290000,batch=218\n",
      "17099 :loss:0.134026, reward_bound:0.150095, reward_mean:0.390000,batch=220\n",
      "17199 :loss:0.142676, reward_bound:0.387420, reward_mean:0.330000,batch=223\n",
      "17299 :loss:0.143555, reward_bound:0.361640, reward_mean:0.280000,batch=229\n",
      "17399 :loss:0.142809, reward_bound:0.430467, reward_mean:0.200000,batch=229\n",
      "17499 :loss:0.135054, reward_bound:0.079766, reward_mean:0.250000,batch=203\n",
      "17599 :loss:0.134699, reward_bound:0.282430, reward_mean:0.270000,batch=225\n",
      "17699 :loss:0.149350, reward_bound:0.205891, reward_mean:0.280000,batch=219\n",
      "17799 :loss:0.151853, reward_bound:0.315554, reward_mean:0.330000,batch=221\n",
      "17899 :loss:0.145910, reward_bound:0.254187, reward_mean:0.310000,batch=223\n",
      "17999 :loss:0.125970, reward_bound:0.205891, reward_mean:0.400000,batch=218\n",
      "18099 :loss:0.098534, reward_bound:0.047101, reward_mean:0.370000,batch=196\n",
      "18199 :loss:0.093399, reward_bound:0.478297, reward_mean:0.300000,batch=107\n",
      "18299 :loss:0.096195, reward_bound:0.288706, reward_mean:0.280000,batch=227\n",
      "18399 :loss:0.080157, reward_bound:0.282430, reward_mean:0.350000,batch=226\n",
      "18499 :loss:0.086556, reward_bound:0.478297, reward_mean:0.360000,batch=230\n",
      "18599 :loss:0.086121, reward_bound:0.228768, reward_mean:0.390000,batch=211\n",
      "18699 :loss:0.092404, reward_bound:0.360301, reward_mean:0.230000,batch=225\n",
      "18799 :loss:0.115633, reward_bound:0.348678, reward_mean:0.330000,batch=178\n",
      "18899 :loss:0.098645, reward_bound:0.400526, reward_mean:0.280000,batch=230\n",
      "18999 :loss:0.093154, reward_bound:0.273957, reward_mean:0.400000,batch=224\n",
      "19099 :loss:0.098404, reward_bound:0.254187, reward_mean:0.350000,batch=212\n",
      "19199 :loss:0.088464, reward_bound:0.262659, reward_mean:0.350000,batch=225\n",
      "19299 :loss:0.093343, reward_bound:0.166772, reward_mean:0.420000,batch=216\n",
      "19399 :loss:0.094169, reward_bound:0.331245, reward_mean:0.260000,batch=228\n",
      "19499 :loss:0.090157, reward_bound:0.379672, reward_mean:0.300000,batch=229\n",
      "19599 :loss:0.081267, reward_bound:0.058150, reward_mean:0.450000,batch=213\n",
      "19699 :loss:0.080809, reward_bound:0.313811, reward_mean:0.260000,batch=216\n",
      "19799 :loss:0.067589, reward_bound:0.282430, reward_mean:0.310000,batch=227\n",
      "19899 :loss:0.076059, reward_bound:0.313811, reward_mean:0.440000,batch=211\n",
      "19999 :loss:0.089075, reward_bound:0.345192, reward_mean:0.410000,batch=227\n",
      "20000 :loss:0.088689, reward_bound:0.348678, reward_mean:0.270000,batch=228\n"
     ]
    }
   ],
   "source": [
    "#env = gym.envs.toy_text.frozen_lake.FrozenLakeEnv(is_slippery=False)\n",
    "obs = env.reset()\n",
    "net = Net(16,HIDDEN,env.action_space.n)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimzer = torch.optim.Adam(params=net.parameters(),lr=1e-3)\n",
    "full_batch = []\n",
    "random.seed(12345)\n",
    "for iter_no,batch in enumerate(iterate_batch(env,net,BATCHS_SIZE)):\n",
    "    reward_mean = np.mean(list(map(lambda x:x.reward ,batch)))\n",
    "    full_batch ,train_obs,train_act,reward_bound = filter_batch(full_batch+batch,PERCERTILE,DISCOUNT)\n",
    "    if 0 == len(full_batch):\n",
    "        continue\n",
    "    #print(train_obs)\n",
    "    obs_v = torch.FloatTensor(train_obs)\n",
    "    act_v = torch.LongTensor(train_act)\n",
    "    full_batch = full_batch[-500:]\n",
    "    \n",
    "    #print(act_v.shape)\n",
    "    optimzer.zero_grad()\n",
    "    action_score = net(obs_v)\n",
    "    #print(action_score.shape) \n",
    "    \n",
    "    loss_v = loss(action_score,act_v)\n",
    "    loss_v.backward()\n",
    "    optimzer.step()\n",
    "    if (iter_no+1)% 100 == 0:\n",
    "        print(\"%d :loss:%f, reward_bound:%f, reward_mean:%f,batch=%d\"%(iter_no,loss_v.item(),reward_bound,reward_mean,len(full_batch)))\n",
    "    writer.add_scalar('loss',loss_v.item(),global_step=iter_no)\n",
    "    writer.add_scalar('reward_bound',reward_bound)\n",
    "    writer.add_scalar('reward_mean',reward_mean)\n",
    "    if 20000 == iter_no:\n",
    "        print(\"%d :loss:%f, reward_bound:%f, reward_mean:%f,batch=%d\"%(iter_no,loss_v.item(),reward_bound,reward_mean,len(full_batch)))\n",
    "        writer.close()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
